{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydot\n",
      "  Downloading https://files.pythonhosted.org/packages/33/d1/b1479a770f66d962f545c2101630ce1d5592d90cb4f083d38862e93d16d2/pydot-1.4.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /opt/conda/lib/python3.7/site-packages (from pydot) (2.4.6)\n",
      "Installing collected packages: pydot\n",
      "Successfully installed pydot-1.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyvis\n",
      "  Downloading https://files.pythonhosted.org/packages/7d/7e/df88acbe771afb1fe69b64516d2a56be46befab3e04cfd4258dc6063f96a/pyvis-0.1.7.0-py3-none-any.whl\n",
      "Requirement already satisfied: jinja2>=2.9.6 in /opt/conda/lib/python3.7/site-packages (from pyvis) (2.10.1)\n",
      "Requirement already satisfied: ipython>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from pyvis) (7.5.0)\n",
      "Requirement already satisfied: networkx>=1.11 in /opt/conda/lib/python3.7/site-packages (from pyvis) (2.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.7/site-packages (from jinja2>=2.9.6->pyvis) (1.1.1)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.7/site-packages (from ipython>=5.3.0->pyvis) (2.4.0)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from ipython>=5.3.0->pyvis) (2.0.9)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.7/site-packages (from ipython>=5.3.0->pyvis) (0.1.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from ipython>=5.3.0->pyvis) (4.4.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /opt/conda/lib/python3.7/site-packages (from ipython>=5.3.0->pyvis) (4.3.2)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.7/site-packages (from ipython>=5.3.0->pyvis) (0.7.5)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /opt/conda/lib/python3.7/site-packages (from ipython>=5.3.0->pyvis) (4.7.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.7/site-packages (from ipython>=5.3.0->pyvis) (41.0.1)\n",
      "Requirement already satisfied: jedi>=0.10 in /opt/conda/lib/python3.7/site-packages (from ipython>=5.3.0->pyvis) (0.13.3)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=5.3.0->pyvis) (0.1.7)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=5.3.0->pyvis) (1.12.0)\n",
      "Requirement already satisfied: ipython_genutils in /opt/conda/lib/python3.7/site-packages (from traitlets>=4.2->ipython>=5.3.0->pyvis) (0.2.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect; sys_platform != \"win32\"->ipython>=5.3.0->pyvis) (0.6.0)\n",
      "Requirement already satisfied: parso>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.10->ipython>=5.3.0->pyvis) (0.4.0)\n",
      "Installing collected packages: pyvis\n",
      "Successfully installed pyvis-0.1.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# basic data libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# scikit-learn modules for pipelining, transformation, model fitting and classification\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# nltk-modules for text processing, tokenizing and lemmatizing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Download relevant ntlk packages\n",
    "nltk.download([\"punkt\", \"stopwords\", \"wordnet\"])\n",
    "\n",
    "# pickle for python object serialization and storing\n",
    "import pickle\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "import pydot\n",
    "import networkx\n",
    "import pyvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "\n",
    "engine = create_engine('sqlite:///crisisresponse.db')\n",
    "df = pd.read_sql_table('messages', engine)\n",
    "X = df.loc[:,\"message\"]\n",
    "Y = df.iloc[:,4:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenize, lemmatize, lower and remove punctuation of input text.\n",
    "\n",
    "    Input arguments:\n",
    "        text: Single string with input text \n",
    "              Example: 'For today:= this is, a advanced _ example #- String!'\n",
    "              \n",
    "    Output:\n",
    "        output: List of processed string\n",
    "                Example: ['today', 'advanced', 'example', 'string']\n",
    "        \n",
    "    \"\"\"\n",
    "    # set text to lower case and remove punctuation\n",
    "    text = re.sub(\"[\\W_]\", \" \", text)\n",
    "    text= text.lower()\n",
    "\n",
    "    # tokenize words \n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # init and remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    output = [lemmatizer.lemmatize(w) for w in tokens if not w in stop_words]\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tok_message\"] = df[\"message\"].apply(tokenize)\n",
    "df_tok = df.explode(\"tok_message\").drop(columns=[\"message\",\"original\",\"id\"])\n",
    "df_tok = df_tok[df_tok[\"related\"] == 1]\n",
    "df_tok.drop(columns=[\"related\"], inplace=True)\n",
    "df_tok = df_tok.reset_index().drop(columns=[\"index\"]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(312195, 38)"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tok.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_columns = df_tok.columns[2:36]\n",
    "df_tok_melt = df_tok.melt(id_vars = ['index', 'tok_message'], value_vars=var_columns)\n",
    "df_tok_melt = df_tok_melt[df_tok_melt[\"value\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1230331, 4)"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tok_melt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "from matplotlib import colors\n",
    "from scipy import interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_interpol = interpolate.interp1d([0, 15], [0,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Network(\"1000px\", \"1500px\", notebook=True)\n",
    "g.hrepulsion(central_gravity=6.55, spring_length=620, node_distance=465, damping=1)\n",
    "#g.add_nodes([1,2,3], value=[10, 100, 400], title=[\"I am node 1\", \"node 2 here\", \"and im node 3\"], label=[\"NODE 1\", \"NODE 2\", \"NODE 3\"], color=[\"#00ff1e\", \"#162347\", \"#dd4b39\"])\n",
    "cat_dict = {}\n",
    "node_index = 0\n",
    "for cat, value in df_tok_melt[\"variable\"].value_counts().sort_values(ascending=False)[:15].iteritems():\n",
    "    node_index += 1\n",
    "    cat_dict[cat] = node_index\n",
    "    g.add_node(cat_dict[cat], value=value*10000, title=\"Category {}: {}\".format(cat,str(value)), label=cat )\n",
    "\n",
    "    \n",
    "word_dict = {}\n",
    "for word, value in df_tok_melt[\"tok_message\"].value_counts().sort_values(ascending=False)[:100].iteritems():\n",
    "    node_index += 1\n",
    "    word_dict[word] = node_index\n",
    "    g.add_node(word_dict[word], value=value*1000, title=\"Word: {}: {}\".format(word,str(value)), color=\"red\",label=word)\n",
    "\n",
    "word_per_cat = pd.DataFrame(df_tok_melt.groupby(\"tok_message\")[\"variable\"].value_counts()).T\n",
    "    \n",
    "edge_dict = {}\n",
    "for word in word_dict.keys():\n",
    "    for cat in word_per_cat[word].columns:\n",
    "        try:\n",
    "            g.add_edge(cat_dict[cat], word_dict[word], width= int(word_per_cat[word][cat][\"variable\"]/50),color = colors.to_hex(cm.get_cmap(\"rainbow\")(cat_interpol(cat_dict[cat]))))\n",
    "        except:\n",
    "            pass\n",
    "        #\n",
    "#g.show_buttons(filter_=['physics'])\n",
    "\n",
    "g.save_graph(\"graph_disaster_response.html\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df.iloc[:,5:].columns\n",
    "df_melt = df.melt(id_vars = ['message'], value_vars=var_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Weather update - a cold front from Cuba that c...</td>\n",
       "      <td>request</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Is the Hurricane over or is it not over</td>\n",
       "      <td>request</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Looking for someone but no name</td>\n",
       "      <td>request</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>request</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>says: west side of Haiti, rest of the country ...</td>\n",
       "      <td>request</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message variable  value\n",
       "0  Weather update - a cold front from Cuba that c...  request      0\n",
       "1            Is the Hurricane over or is it not over  request      0\n",
       "2                    Looking for someone but no name  request      0\n",
       "3  UN reports Leogane 80-90 destroyed. Only Hospi...  request      1\n",
       "4  says: west side of Haiti, rest of the country ...  request      0"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_melt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(884952, 42)"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([df,df_melt[\"variable\"]], axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26028, 41)"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(884952, 3)"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_melt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "bla = df_melt[df_melt[\"value\"] == 1][\"variable\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['aid_related', 'weather_related', 'request', 'other_aid', 'food',\n",
       "       'earthquake', 'storm', 'shelter', 'floods', 'medical_help',\n",
       "       'infrastructure_related', 'water', 'other_weather', 'buildings',\n",
       "       'medical_products', 'transport', 'death', 'other_infrastructure',\n",
       "       'refugees', 'military', 'search_and_rescue', 'money', 'electricity',\n",
       "       'cold', 'security', 'clothing', 'aid_centers', 'missing_people',\n",
       "       'hospitals', 'fire', 'tools', 'shops', 'offer'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bla.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genre\n",
       "direct    10634\n",
       "news      13036\n",
       "social     2358\n",
       "Name: message, dtype: int64"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('genre').count()['message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
