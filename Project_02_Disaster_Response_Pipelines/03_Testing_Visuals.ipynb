{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydot\n",
      "  Downloading https://files.pythonhosted.org/packages/33/d1/b1479a770f66d962f545c2101630ce1d5592d90cb4f083d38862e93d16d2/pydot-1.4.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /opt/conda/lib/python3.7/site-packages (from pydot) (2.4.6)\n",
      "Installing collected packages: pydot\n",
      "Successfully installed pydot-1.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyvis\n",
      "  Downloading https://files.pythonhosted.org/packages/7d/7e/df88acbe771afb1fe69b64516d2a56be46befab3e04cfd4258dc6063f96a/pyvis-0.1.7.0-py3-none-any.whl\n",
      "Requirement already satisfied: jinja2>=2.9.6 in /opt/conda/lib/python3.7/site-packages (from pyvis) (2.10.1)\n",
      "Requirement already satisfied: ipython>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from pyvis) (7.5.0)\n",
      "Requirement already satisfied: networkx>=1.11 in /opt/conda/lib/python3.7/site-packages (from pyvis) (2.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.7/site-packages (from jinja2>=2.9.6->pyvis) (1.1.1)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.7/site-packages (from ipython>=5.3.0->pyvis) (2.4.0)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from ipython>=5.3.0->pyvis) (2.0.9)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.7/site-packages (from ipython>=5.3.0->pyvis) (0.1.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from ipython>=5.3.0->pyvis) (4.4.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /opt/conda/lib/python3.7/site-packages (from ipython>=5.3.0->pyvis) (4.3.2)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.7/site-packages (from ipython>=5.3.0->pyvis) (0.7.5)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /opt/conda/lib/python3.7/site-packages (from ipython>=5.3.0->pyvis) (4.7.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.7/site-packages (from ipython>=5.3.0->pyvis) (41.0.1)\n",
      "Requirement already satisfied: jedi>=0.10 in /opt/conda/lib/python3.7/site-packages (from ipython>=5.3.0->pyvis) (0.13.3)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=5.3.0->pyvis) (0.1.7)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=5.3.0->pyvis) (1.12.0)\n",
      "Requirement already satisfied: ipython_genutils in /opt/conda/lib/python3.7/site-packages (from traitlets>=4.2->ipython>=5.3.0->pyvis) (0.2.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect; sys_platform != \"win32\"->ipython>=5.3.0->pyvis) (0.6.0)\n",
      "Requirement already satisfied: parso>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.10->ipython>=5.3.0->pyvis) (0.4.0)\n",
      "Installing collected packages: pyvis\n",
      "Successfully installed pyvis-0.1.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# basic data libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# scikit-learn modules for pipelining, transformation, model fitting and classification\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# nltk-modules for text processing, tokenizing and lemmatizing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download relevant ntlk packages\n",
    "nltk.download([\"punkt\", \"stopwords\", \"wordnet\"])\n",
    "\n",
    "# pickle for python object serialization and storing\n",
    "import pickle\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "import pydot\n",
    "import networkx\n",
    "import pyvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "\n",
    "engine = create_engine('sqlite:///crisisresponse.db')\n",
    "df = pd.read_sql_table('messages', engine)\n",
    "X = df.loc[:,\"message\"]\n",
    "Y = df.iloc[:,4:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"message\"].apply(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvis.network import Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"500px\"\n",
       "            height=\"500px\"\n",
       "            src=\"blah.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f4a2c484ef0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = Network(notebook=True)\n",
    "g.add_nodes([1,2,3], value=[10, 100, 400], title=[\"I am node 1\", \"node 2 here\", \"and im node 3\"], label=[\"NODE 1\", \"NODE 2\", \"NODE 3\"], color=[\"#00ff1e\", \"#162347\", \"#dd4b39\"])\n",
    "g.show(\"blah.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.toggle_physics(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenize, lemmatize, lower and remove punctuation of input text.\n",
    "\n",
    "    Input arguments:\n",
    "        text: Single string with input text \n",
    "              Example: 'For today:= this is, a advanced _ example #- String!'\n",
    "              \n",
    "    Output:\n",
    "        output: List of processed string\n",
    "                Example: ['today', 'advanced', 'example', 'string']\n",
    "        \n",
    "    \"\"\"\n",
    "    # set text to lower case and remove punctuation\n",
    "    text = re.sub(\"[\\W_]\", \" \", text)\n",
    "    text= text.lower()\n",
    "\n",
    "    # tokenize words \n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # init and remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    output = [lemmatizer.lemmatize(w) for w in tokens if not w in stop_words]\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
