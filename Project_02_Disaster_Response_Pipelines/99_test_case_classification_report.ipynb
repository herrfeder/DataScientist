{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_multilabel_classification(n_classes=36, random_state=0, n_samples=100000)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of all elements: \n",
      "\t X_train: (70000, 20)\n",
      "\t X_test: (30000, 20)\n",
      "\t Y_train: (70000, 36)\n",
      "\t Y_test: (30000, 36)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"\"\"\n",
    "Shape of all elements: \\n\\t X_train: {}\n",
    "\\t X_test: {}\n",
    "\\t Y_train: {}\n",
    "\\t Y_test: {}\"\"\".format(\n",
    "        X_train.shape, X_test.shape, Y_train.shape, Y_test.shape\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultiOutputClassifier(RandomForestClassifier(n_estimators=10)).fit(X_train, Y_train)\n",
    "Y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 36)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 36)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.14      0.24      1641\n",
      "           1       0.67      0.08      0.14      2183\n",
      "           2       0.86      0.15      0.26      1855\n",
      "           3       0.89      0.14      0.25      1612\n",
      "           4       0.75      0.04      0.07      1334\n",
      "           5       0.83      0.14      0.24      1850\n",
      "           6       0.81      0.06      0.11      1374\n",
      "           7       0.82      0.22      0.34      2604\n",
      "           8       0.76      0.15      0.25      2906\n",
      "           9       0.79      0.03      0.06      1150\n",
      "          10       0.81      0.17      0.28      2355\n",
      "          11       0.87      0.13      0.23      1650\n",
      "          12       0.85      0.11      0.20      1723\n",
      "          13       0.78      0.17      0.27      2659\n",
      "          14       0.67      0.01      0.02       203\n",
      "          15       0.92      0.04      0.08       285\n",
      "          16       0.00      0.00      0.00        58\n",
      "          17       0.76      0.16      0.26      2450\n",
      "          18       0.83      0.23      0.36      2293\n",
      "          19       0.81      0.18      0.29      2492\n",
      "          20       0.81      0.19      0.31      2883\n",
      "          21       0.85      0.19      0.31      2333\n",
      "          22       0.88      0.14      0.24      1446\n",
      "          23       0.81      0.17      0.29      2401\n",
      "          24       1.00      0.06      0.12       337\n",
      "          25       0.88      0.28      0.43      1930\n",
      "          26       0.50      0.00      0.00       449\n",
      "          27       0.84      0.24      0.37      2829\n",
      "          28       0.75      0.03      0.05      1554\n",
      "          29       0.78      0.04      0.08      1239\n",
      "          30       0.88      0.07      0.13       823\n",
      "          31       0.82      0.23      0.36      2200\n",
      "          32       0.77      0.07      0.13      1382\n",
      "          33       0.85      0.13      0.23      1735\n",
      "          34       0.00      0.00      0.00        53\n",
      "          35       0.75      0.07      0.13      1914\n",
      "\n",
      "   micro avg       0.82      0.15      0.25     60185\n",
      "   macro avg       0.76      0.12      0.20     60185\n",
      "weighted avg       0.81      0.15      0.25     60185\n",
      " samples avg       0.29      0.24      0.25     60185\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
